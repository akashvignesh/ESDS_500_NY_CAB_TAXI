{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "#from src.config import TRANSFORMED_DATA_DIR\n",
    "#from src.data_utils import split_time_series_data\n",
    "#from src.experiment_utils import set_mlflow_tracking  # Assuming this sets up MLflow\n",
    "from dotenv import load_dotenv\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from prophet import Prophet\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Load data\n",
    "df = pd.read_parquet(\"D:/Code/Git/ESDS_500_NY_CAB_TAXI/data/transformed/tabular_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mlflow\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[43mDATA_FILE\u001b[49m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Split data\u001b[39;00m\n\u001b[0;32m     23\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m split_time_series_data(\n\u001b[0;32m     24\u001b[0m     df,\n\u001b[0;32m     25\u001b[0m     cutoff_date\u001b[38;5;241m=\u001b[39mdatetime(\u001b[38;5;241m2023\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     26\u001b[0m     target_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DATA_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "def split_time_series_data(df, cutoff_date, target_column):\n",
    "    \"\"\"Split DataFrame into train and test sets based on a cutoff date.\"\"\"\n",
    "    df['datetime'] = pd.to_datetime(df.index)  # Assuming index is datetime; adjust if needed\n",
    "    train_df = df[df['datetime'] < cutoff_date]\n",
    "    test_df = df[df['datetime'] >= cutoff_date]\n",
    "    X_train = train_df.drop(columns=[target_column, 'datetime'])\n",
    "    y_train = train_df[target_column]\n",
    "    X_test = test_df.drop(columns=[target_column, 'datetime'])\n",
    "    y_test = test_df[target_column]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Mock set_mlflow_tracking function\n",
    "def set_mlflow_tracking():\n",
    "    \"\"\"Set up MLflow tracking (mocked to local).\"\"\"\n",
    "    mlflow.set_tracking_uri(\"file:./mlruns\")  # Local tracking; adjust if using a server\n",
    "    mlflow.set_experiment(\"Prophet_Experiment\")\n",
    "    return mlflow\n",
    "\n",
    "# Load data\n",
    "df = pd.read_parquet(\"D:/Code/Git/ESDS_500_NY_CAB_TAXI/data/transformed/tabular_data.parquet\")\n",
    "\n",
    "# Split data\n",
    "X_train, y_train, X_test, y_test = split_time_series_data(\n",
    "    df,\n",
    "    cutoff_date=datetime(2023, 9, 1, 0, 0, 0),\n",
    "    target_column=\"target\"\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Ensure y_train and y_test have a datetime index\n",
    "if not isinstance(y_train.index, pd.DatetimeIndex):\n",
    "    y_train.index = pd.date_range(start=\"2022-01-01\", periods=len(y_train), freq=\"D\")\n",
    "if not isinstance(y_test.index, pd.DatetimeIndex):\n",
    "    y_test.index = pd.date_range(start=y_train.index[-1] + pd.Timedelta(days=1), \n",
    "                                 periods=len(y_test), freq=\"D\")\n",
    "\n",
    "# Prepare data for Prophet (requires 'ds' and 'y' columns)\n",
    "train_df = pd.DataFrame({\n",
    "    'ds': y_train.index,\n",
    "    'y': y_train.values\n",
    "})\n",
    "test_df = pd.DataFrame({\n",
    "    'ds': y_test.index,\n",
    "    'y': y_test.values\n",
    "})\n",
    "\n",
    "# Train the Prophet model\n",
    "prophet_model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=True\n",
    ")\n",
    "prophet_model.fit(train_df)\n",
    "\n",
    "# Make future dataframe for predictions\n",
    "future = prophet_model.make_future_dataframe(periods=len(y_test), freq=\"D\")\n",
    "forecast = prophet_model.predict(future)\n",
    "\n",
    "# Extract predictions for the test period\n",
    "predictions = forecast.tail(len(y_test))['yhat'].values\n",
    "\n",
    "# Compute Mean Absolute Error (MAE)\n",
    "test_mae = mean_absolute_error(y_test, predictions)\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Set up MLflow\n",
    "load_dotenv()\n",
    "mlflow = set_mlflow_tracking()\n",
    "\n",
    "# Custom function to log Prophet model to MLflow\n",
    "def log_prophet_to_mlflow(model, experiment_name, metric_name, score, test_df):\n",
    "    with mlflow.start_run():\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_param(\"yearly_seasonality\", True)\n",
    "        mlflow.log_param(\"weekly_seasonality\", True)\n",
    "        mlflow.log_param(\"daily_seasonality\", True)\n",
    "        \n",
    "        # Log metric\n",
    "        mlflow.log_metric(metric_name, score)\n",
    "        \n",
    "        # Use test_df['ds'] as input for signature inference\n",
    "        input_df = test_df[['ds']]\n",
    "        predictions = model.predict(input_df)['yhat']\n",
    "        \n",
    "        # Infer signature\n",
    "        signature = infer_signature(input_df, predictions)\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.prophet.log_model(model, \"model\", signature=signature)\n",
    "\n",
    "# Log the Prophet model\n",
    "log_prophet_to_mlflow(prophet_model, \"Prophet\", \"mean_absolute_error\", test_mae, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonTestEnvForPy11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
